{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "require input \"X_train_rotate.npy\"\n",
    "\"X_test.npy\"\n",
    "\"target_train.npy\"\n",
    "\"test_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 20\n",
    "split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rotate=np.load('X_train_rotate.npy')\n",
    "target_train=np.load('target_train.npy')\n",
    "X_test=np.load('X_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(filepath, patience):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "def train_top_model(input_shape,drop,dense1,dense2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(dense1, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(dense2, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    mypotim=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=mypotim,\n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'top_model_weights_path.h5'\n",
    "#callbacks = get_callbacks(file_path,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.244580787342 acc 0.897635749561\n",
      "testing: loss 0.373336014548 acc 0.815160955348\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.202676654683 acc 0.904910366329\n",
      "testing: loss 0.343149097847 acc 0.85254413298\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.207498256132 acc 0.910366328932\n",
      "testing: loss 0.309746395359 acc 0.85254413298\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.217370237375 acc 0.909350649289\n",
      "testing: loss 0.338682748176 acc 0.841995841996\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.191604627081 acc 0.917164372968\n",
      "testing: loss 0.322933845943 acc 0.849115504683\n",
      "drop= 0.3 dense1= 1024 dense2= 512 Test loss average================ 0.337569620375 Test loss std: 0.021447341644\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.205458445878 acc 0.916082099247\n",
      "testing: loss 0.360608060598 acc 0.829698857736\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.20826638645 acc 0.906988828267\n",
      "testing: loss 0.33123254612 acc 0.849428868182\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.189909800643 acc 0.917900753442\n",
      "testing: loss 0.326492376525 acc 0.85046728972\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.226834601148 acc 0.906233766234\n",
      "testing: loss 0.334166787332 acc 0.839916839917\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.189480375621 acc 0.921578810683\n",
      "testing: loss 0.313222687712 acc 0.870967741935\n",
      "drop= 0.3 dense1= 1024 dense2= 256 Test loss average================ 0.333144491657 Test loss std: 0.0154924313864\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.225450390533 acc 0.905429981829\n",
      "testing: loss 0.368763161597 acc 0.817237798546\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.216507337281 acc 0.908807482463\n",
      "testing: loss 0.335042704179 acc 0.846313603354\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.212292734707 acc 0.91582229152\n",
      "testing: loss 0.309049578997 acc 0.850467289782\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.207718004592 acc 0.912987013018\n",
      "testing: loss 0.337779778639 acc 0.836798336798\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.206054872621 acc 0.913528953503\n",
      "testing: loss 0.324821776536 acc 0.853277835588\n",
      "drop= 0.3 dense1= 512 dense2= 512 Test loss average================ 0.33509139999 Test loss std: 0.0196196783219\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.240181583779 acc 0.897116134076\n",
      "testing: loss 0.370837452256 acc 0.817237798546\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.220761923534 acc 0.905429981813\n",
      "testing: loss 0.333338421608 acc 0.860851505773\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.188023941126 acc 0.92153806185\n",
      "testing: loss 0.311743418561 acc 0.860851505711\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.245331230086 acc 0.896623376561\n",
      "testing: loss 0.340237328763 acc 0.83367983368\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.155423624249 acc 0.941313944415\n",
      "testing: loss 0.321406814658 acc 0.857440166493\n",
      "drop= 0.3 dense1= 512 dense2= 256 Test loss average================ 0.335512687169 Test loss std: 0.0201932784203\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.248879012631 acc 0.894777864396\n",
      "testing: loss 0.363146275406 acc 0.829698857736\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.205846605504 acc 0.912704598597\n",
      "testing: loss 0.341418382782 acc 0.852544132918\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.239594863712 acc 0.893738633427\n",
      "testing: loss 0.316521352728 acc 0.853582554579\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.219833469762 acc 0.907012986951\n",
      "testing: loss 0.328537608011 acc 0.848232848233\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.227711864599 acc 0.899766294453\n",
      "testing: loss 0.328024896971 acc 0.861602497399\n",
      "drop= 0.5 dense1= 1024 dense2= 512 Test loss average================ 0.33552970318 Test loss std: 0.0158989339703\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.255210403356 acc 0.8929592102\n",
      "testing: loss 0.367417435223 acc 0.811007268951\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.242603891672 acc 0.899454403741\n",
      "testing: loss 0.324065396719 acc 0.859813084143\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.192671807056 acc 0.920239023138\n",
      "testing: loss 0.315194177881 acc 0.856697819377\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.240225607025 acc 0.898701298639\n",
      "testing: loss 0.33473905035 acc 0.830561330561\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.234074114058 acc 0.896650220706\n",
      "testing: loss 0.323413547253 acc 0.860561914672\n",
      "drop= 0.5 dense1= 1024 dense2= 256 Test loss average================ 0.332965921485 Test loss std: 0.0183103951354\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.245079435778 acc 0.899714211499\n",
      "testing: loss 0.373991039312 acc 0.809968847352\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.201545117288 acc 0.916341906989\n",
      "testing: loss 0.330620580559 acc 0.854620976147\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.258748635266 acc 0.888022863097\n",
      "testing: loss 0.32323235849 acc 0.844236760125\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.195031967055 acc 0.919220779159\n",
      "testing: loss 0.3310195935 acc 0.850311850312\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.189137463278 acc 0.920540119434\n",
      "testing: loss 0.320166615173 acc 0.864724245578\n",
      "drop= 0.5 dense1= 512 dense2= 512 Test loss average================ 0.335806037407 Test loss std: 0.0195477994045\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.208342397173 acc 0.914783060551\n",
      "testing: loss 0.360671202218 acc 0.83800623053\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.257979041196 acc 0.88672382437\n",
      "testing: loss 0.340503560234 acc 0.848390446552\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.267989084175 acc 0.885424785674\n",
      "testing: loss 0.329470339872 acc 0.847352024922\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.176812472955 acc 0.929610389548\n",
      "testing: loss 0.331195774668 acc 0.85446985447\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.198391281716 acc 0.912749935066\n",
      "testing: loss 0.318673947195 acc 0.860561914672\n",
      "drop= 0.5 dense1= 512 dense2= 256 Test loss average================ 0.336102964838 Test loss std: 0.0141053636117\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.253640047155 acc 0.896076903107\n",
      "testing: loss 0.362895564947 acc 0.823468328141\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.237602127039 acc 0.899454403741\n",
      "testing: loss 0.333351866417 acc 0.849428868151\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.211222530352 acc 0.911145752159\n",
      "testing: loss 0.318813339851 acc 0.855659397715\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.249370972839 acc 0.897142857081\n",
      "testing: loss 0.328350484123 acc 0.85446985447\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.205281148736 acc 0.913269280691\n",
      "testing: loss 0.319413407294 acc 0.865764828304\n",
      "drop= 0.7 dense1= 1024 dense2= 512 Test loss average================ 0.332564932526 Test loss std: 0.0161269775245\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.248701083998 acc 0.897895557303\n",
      "testing: loss 0.359834571401 acc 0.821391484943\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.249907148948 acc 0.894518056638\n",
      "testing: loss 0.340899304995 acc 0.843198338556\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.268296691559 acc 0.879968823086\n",
      "testing: loss 0.329257181717 acc 0.845275181724\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.197266494268 acc 0.922857142795\n",
      "testing: loss 0.3231845179 acc 0.844074844075\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.236139327125 acc 0.896909893519\n",
      "testing: loss 0.322968546533 acc 0.860561914672\n",
      "drop= 0.7 dense1= 1024 dense2= 256 Test loss average================ 0.335228824509 Test loss std: 0.0139186018294\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.217319757656 acc 0.914003637324\n",
      "testing: loss 0.368016667579 acc 0.820353063344\n",
      "\n",
      "===================FOLD= 1\n",
      "training: loss 0.22230657783 acc 0.906988828267\n",
      "testing: loss 0.343798551047 acc 0.853582554548\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.236493735309 acc 0.903871135375\n",
      "testing: loss 0.323028751623 acc 0.851505711319\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.220795060319 acc 0.913766233797\n",
      "testing: loss 0.327143290682 acc 0.839916839917\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.208296877576 acc 0.910672552568\n",
      "testing: loss 0.317891785778 acc 0.86680541103\n",
      "drop= 0.7 dense1= 512 dense2= 512 Test loss average================ 0.335975809342 Test loss std: 0.0182235204176\n",
      "\n",
      "===================FOLD= 0\n",
      "training: loss 0.246296015853 acc 0.895037672138\n",
      "testing: loss 0.366401076843 acc 0.823468328141\n",
      "\n",
      "===================FOLD= 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: loss 0.244871203951 acc 0.89815536503\n",
      "testing: loss 0.338804580836 acc 0.845275181755\n",
      "\n",
      "===================FOLD= 2\n",
      "training: loss 0.227861959707 acc 0.904650558602\n",
      "testing: loss 0.322893172781 acc 0.854620976178\n",
      "\n",
      "===================FOLD= 3\n",
      "training: loss 0.2101080586 acc 0.9111688312\n",
      "testing: loss 0.326880704217 acc 0.836798336798\n",
      "\n",
      "===================FOLD= 4\n",
      "training: loss 0.211975410979 acc 0.91197091663\n",
      "testing: loss 0.318037865901 acc 0.863683662851\n",
      "drop= 0.7 dense1= 512 dense2= 256 Test loss average================ 0.334603480116 Test loss std: 0.0173200955231\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "X_train = model.predict(X_train_rotate)\n",
    "input_shape=X_train.shape[1:]\n",
    "\n",
    "#5fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "for drop in [0.3,0.5,0.7]:\n",
    "    for dense1 in [1024,512]:\n",
    "        for dense2 in [512,256]:\n",
    "            cvscores = []\n",
    "            cvscores_train = []\n",
    "            j=0\n",
    "            for train, test in kfold.split(X_train, target_train):\n",
    "                print('\\n===================FOLD=',j)\n",
    "                model = train_top_model(input_shape,drop,dense1,dense2)\n",
    "                model.fit(X_train[train], target_train[train],\n",
    "                           epochs=epochs,\n",
    "                           batch_size=batch_size,\n",
    "                           verbose=0,\n",
    "                           validation_data=(X_train[test], target_train[test]),\n",
    "                           callbacks=get_callbacks(file_path, patience=5))\n",
    "                model.load_weights(filepath=file_path)\n",
    "                scores = model.evaluate(X_train[test], target_train[test], verbose=0)\n",
    "                scores_train = model.evaluate(X_train[train], target_train[train], verbose=0)\n",
    "                print('training:', model.metrics_names[0], scores_train[0], model.metrics_names[1], scores_train[1])\n",
    "                print('testing:', model.metrics_names[0], scores[0], model.metrics_names[1], scores[1])\n",
    "                cvscores.append(scores[0])\n",
    "                cvscores_train.append(scores_train[0])\n",
    "                j=j+1\n",
    "            \n",
    "            print('drop=',drop,'dense1=',dense1,'dense2=',dense2,'Test loss average================',np.mean(cvscores), 'Test loss std:',np.std(cvscores))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('bottleneck_features_test.npy')\n",
    "predicted_test=model.predict(X_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission = pd.DataFrame()\n",
    "test_id=np.load('test_id.npy')\n",
    "submission['id']=test_id\n",
    "submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
