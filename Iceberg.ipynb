{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "from scipy.ndimage.filters import median_filter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rotate = np.load(\"/home/ubuntu/X_train_rotate.npy\")\n",
    "target_train = np.load(\"/home/ubuntu/target_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(filepath, patience):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "file_path = \"model_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.3351856724 acc 0.896767465178\n",
      "testing: loss 0.593873187961 acc 0.771784233602\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.717585067555 acc 0.484880082985\n",
      "testing: loss 0.7368239335 acc 0.473029047622\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.721141030391 acc 0.484375\n",
      "testing: loss 0.73419268926 acc 0.483333333333\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.352697565578 acc 0.843912591051\n",
      "testing: loss 0.543043679398 acc 0.774058577905\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.417890103171 acc 0.832466181061\n",
      "testing: loss 0.550879797926 acc 0.702928871789\n",
      "lr= 0.0001 drop= 0.2 batch_size= 72 loss============== 0.63 (+/- 0.09)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.506176769112 acc 0.758081334972\n",
      "testing: loss 0.54480930359 acc 0.742738590448\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.109811115423 acc 0.96246089683\n",
      "testing: loss 0.388591378371 acc 0.858921162073\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.552029107014 acc 0.764583333333\n",
      "testing: loss 0.603834392627 acc 0.666666666667\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.496081860331 acc 0.772112382934\n",
      "testing: loss 0.592926870205 acc 0.723849372884\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.737559512552 acc 0.423517169615\n",
      "testing: loss 0.74763760856 acc 0.422594142634\n",
      "lr= 0.0001 drop= 0.2 batch_size= 64 loss============== 0.58 (+/- 0.12)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.303129381367 acc 0.885297183697\n",
      "testing: loss 0.560548921344 acc 0.738589213102\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.0152237724078 acc 0.997914494265\n",
      "testing: loss 0.316473034655 acc 0.912863070787\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.603128808737 acc 0.753125\n",
      "testing: loss 0.706487548351 acc 0.675\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.259353447159 acc 0.882414151925\n",
      "testing: loss 0.474710025797 acc 0.820083682757\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.482580191527 acc 0.75650364204\n",
      "testing: loss 0.604489730991 acc 0.715481173044\n",
      "lr= 0.0001 drop= 0.2 batch_size= 48 loss============== 0.53 (+/- 0.13)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.018970570209 acc 0.994786235662\n",
      "testing: loss 0.263392428794 acc 0.883817427386\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.0483549392711 acc 0.994786235662\n",
      "testing: loss 0.312269157672 acc 0.904564315353\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.0139669330015 acc 1.0\n",
      "testing: loss 0.314465785523 acc 0.866666666667\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.0960164374355 acc 0.970863683663\n",
      "testing: loss 0.407130209347 acc 0.824267782926\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.302232037986 acc 0.859521331946\n",
      "testing: loss 0.420904665063 acc 0.794979080745\n",
      "lr= 0.0001 drop= 0.2 batch_size= 24 loss============== 0.34 (+/- 0.06)\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.650018289576 acc 0.595411887818\n",
      "testing: loss 0.635117623569 acc 0.630705395922\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 1.1925441964 acc 0.489051094455\n",
      "testing: loss 1.20409468299 acc 0.489626557995\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.679640026887 acc 0.619791666667\n",
      "testing: loss 0.736919720968 acc 0.6\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.710957389592 acc 0.635796045786\n",
      "testing: loss 0.757504130507 acc 0.619246862922\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.837320760559 acc 0.436004162331\n",
      "testing: loss 0.841865931096 acc 0.435146443889\n",
      "lr= 0.0001 drop= 0.4 batch_size= 72 loss============== 0.84 (+/- 0.20)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.577972574453 acc 0.741397289153\n",
      "testing: loss 0.678629620936 acc 0.705394192603\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.610186121697 acc 0.678832116105\n",
      "testing: loss 0.658932454606 acc 0.647302904688\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.70212577184 acc 0.467708333333\n",
      "testing: loss 0.70832567215 acc 0.466666666667\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.620021236303 acc 0.681581685744\n",
      "testing: loss 0.658966407741 acc 0.6820083687\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.464335469996 acc 0.778355879292\n",
      "testing: loss 0.519041225751 acc 0.753138076311\n",
      "lr= 0.0001 drop= 0.4 batch_size= 64 loss============== 0.64 (+/- 0.07)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.176701763546 acc 0.949947862481\n",
      "testing: loss 0.455746712774 acc 0.792531121074\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.779407852136 acc 0.523461939023\n",
      "testing: loss 0.899737308629 acc 0.443983403108\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.501561254263 acc 0.797916666667\n",
      "testing: loss 0.55657702287 acc 0.75\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.135912761619 acc 0.951092611863\n",
      "testing: loss 0.35519023595 acc 0.861924686691\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.224036460281 acc 0.93756503642\n",
      "testing: loss 0.339027976267 acc 0.83682008443\n",
      "lr= 0.0001 drop= 0.4 batch_size= 48 loss============== 0.52 (+/- 0.20)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.0704148034832 acc 0.982273201251\n",
      "testing: loss 0.238204424868 acc 0.892116183067\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.137080319552 acc 0.948905108557\n",
      "testing: loss 0.273234078065 acc 0.900414937759\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.083908720687 acc 0.980208333333\n",
      "testing: loss 0.248716040701 acc 0.908333333333\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.0952142548363 acc 0.971904266389\n",
      "testing: loss 0.32812015812 acc 0.870292887279\n",
      "lr= 0.0001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.181779819576 acc 0.936524453694\n",
      "testing: loss 0.322305529686 acc 0.841004184849\n",
      "lr= 0.0001 drop= 0.4 batch_size= 24 loss============== 0.28 (+/- 0.04)\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.670207223194 acc 0.618352450842\n",
      "testing: loss 0.668700793472 acc 0.609958507955\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.86883636846 acc 0.489051094455\n",
      "testing: loss 0.87919968243 acc 0.489626557995\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.805164045095 acc 0.632291666667\n",
      "testing: loss 0.804302660624 acc 0.620833333333\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.741798951896 acc 0.443288241415\n",
      "testing: loss 0.735497129263 acc 0.464435147316\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.73066635757 acc 0.536940686785\n",
      "testing: loss 0.719312616721 acc 0.556485356272\n",
      "lr= 0.0001 drop= 0.6 batch_size= 72 loss============== 0.76 (+/- 0.07)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.679258580563 acc 0.601668404961\n",
      "testing: loss 0.694311307426 acc 0.551867220041\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.789915995193 acc 0.488008341588\n",
      "testing: loss 0.814586946331 acc 0.485477178547\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.856725116571 acc 0.489583333333\n",
      "testing: loss 0.867660911878 acc 0.4875\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.682302520129 acc 0.66285119667\n",
      "testing: loss 0.681450711883 acc 0.644351464934\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.579982241424 acc 0.721123829344\n",
      "testing: loss 0.606583933451 acc 0.686192469617\n",
      "lr= 0.0001 drop= 0.6 batch_size= 64 loss============== 0.73 (+/- 0.09)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.561809862928 acc 0.710114702132\n",
      "testing: loss 0.654678473334 acc 0.630705395922\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.713695370181 acc 0.502606881703\n",
      "testing: loss 0.76141973452 acc 0.443983402551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.694584484895 acc 0.654166666667\n",
      "testing: loss 0.682410097122 acc 0.658333333333\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.252506632059 acc 0.895941727367\n",
      "testing: loss 0.337250506653 acc 0.845188285018\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.187306489809 acc 0.924037460978\n",
      "testing: loss 0.312482100276 acc 0.853556486104\n",
      "lr= 0.0001 drop= 0.6 batch_size= 48 loss============== 0.55 (+/- 0.19)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.227934690971 acc 0.916579770657\n",
      "testing: loss 0.269666560087 acc 0.883817427633\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.196728490441 acc 0.912408759186\n",
      "testing: loss 0.263567676157 acc 0.904564315353\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.734842395782 acc 0.476041666667\n",
      "testing: loss 0.792626889547 acc 0.416666666667\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.25457074832 acc 0.896982310094\n",
      "testing: loss 0.399041626182 acc 0.836820083931\n",
      "lr= 0.0001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.193812313938 acc 0.913631633715\n",
      "testing: loss 0.304283893383 acc 0.853556486104\n",
      "lr= 0.0001 drop= 0.6 batch_size= 24 loss============== 0.41 (+/- 0.20)\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.700679033218 acc 0.459854014226\n",
      "testing: loss 0.692867091583 acc 0.485477180402\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.703345699094 acc 0.477580812788\n",
      "testing: loss 0.714297437074 acc 0.414937759645\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.722910716136 acc 0.532291666667\n",
      "testing: loss 0.762452371915 acc 0.541666666667\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.586482710446 acc 0.70551508845\n",
      "testing: loss 0.576321174909 acc 0.707112971459\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.696012196506 acc 0.513007284079\n",
      "testing: loss 0.666368076243 acc 0.556485356521\n",
      "lr= 0.0001 drop= 0.8 batch_size= 72 loss============== 0.68 (+/- 0.06)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.713397129982 acc 0.47236704845\n",
      "testing: loss 0.703638587008 acc 0.481327802808\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.69333761253 acc 0.498435870139\n",
      "testing: loss 0.707544978476 acc 0.419087137548\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.687073860566 acc 0.514583333333\n",
      "testing: loss 0.701474483808 acc 0.475\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.479246641762 acc 0.761706555671\n",
      "testing: loss 0.500676427676 acc 0.769874478234\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.688440575188 acc 0.514047866805\n",
      "testing: loss 0.689265342188 acc 0.535564853681\n",
      "lr= 0.0001 drop= 0.8 batch_size= 64 loss============== 0.66 (+/- 0.08)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.685387373033 acc 0.546402502918\n",
      "testing: loss 0.696630199915 acc 0.551867220041\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.701339896454 acc 0.554744525983\n",
      "testing: loss 0.709624376534 acc 0.510373444602\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.713467858235 acc 0.453125\n",
      "testing: loss 0.740021936099 acc 0.429166666667\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.761258530691 acc 0.483870967742\n",
      "testing: loss 0.768193402051 acc 0.48117154899\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.63348391128 acc 0.672216441207\n",
      "testing: loss 0.623093671629 acc 0.665271968024\n",
      "lr= 0.0001 drop= 0.8 batch_size= 48 loss============== 0.71 (+/- 0.05)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.742067716542 acc 0.486965588814\n",
      "testing: loss 0.731066545758 acc 0.489626557995\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.310135730893 acc 0.864442126346\n",
      "testing: loss 0.343316779963 acc 0.87966805004\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.300741496682 acc 0.861458333333\n",
      "testing: loss 0.319799008965 acc 0.845833333333\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.333986435857 acc 0.838709677419\n",
      "testing: loss 0.364854709499 acc 0.836820084929\n",
      "lr= 0.0001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.734577838017 acc 0.503642039542\n",
      "testing: loss 0.710741269539 acc 0.539748954349\n",
      "lr= 0.0001 drop= 0.8 batch_size= 24 loss============== 0.49 (+/- 0.19)\n",
      "ir= 1e-05\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 1e-05\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.708233122905 acc 0.484880083016\n",
      "testing: loss 0.685421302358 acc 0.556016598994\n",
      "lr= 1e-05\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.0996893576353 acc 0.978102189781\n",
      "testing: loss 0.309159287959 acc 0.858921162073\n",
      "lr= 1e-05\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.728910120328 acc 0.459375\n",
      "testing: loss 0.734930745761 acc 0.470833333333\n",
      "lr= 1e-05\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.698379911195 acc 0.488033298647\n",
      "testing: loss 0.691421507043 acc 0.502092051082\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n\nCaused by op 'training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-9b973f894146>\", line 57, in <module>\n    callbacks=callbacks)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 960, in fit\n    validation_steps=validation_steps)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 1634, in fit\n    self._make_train_function()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 555, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3083, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_169/MaxPool', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-9b973f894146>\", line 24, in <module>\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3466, in pool2d\n    data_format=tf_data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1958, in max_pool\n    name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2806, in _max_pool\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9b973f894146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                            \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_rotate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                            callbacks=callbacks)\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mgmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_rotate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n\nCaused by op 'training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-9b973f894146>\", line 57, in <module>\n    callbacks=callbacks)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 960, in fit\n    validation_steps=validation_steps)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 1634, in fit\n    self._make_train_function()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 555, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3083, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_169/MaxPool', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-9b973f894146>\", line 24, in <module>\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3466, in pool2d\n    data_format=tf_data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1958, in max_pool\n    name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2806, in _max_pool\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle = True, random_state = seed)\n",
    "\n",
    "for lr in [1e-4,1e-5]:\n",
    "    print (\"ir=\",lr)\n",
    "    for drop in [0.2, 0.4, 0.6, 0.8]:\n",
    "        print (\"drop=\",drop)\n",
    "        for batch_size in [72, 64, 48, 24]:\n",
    "            print(\"batch_size=\",batch_size)\n",
    "            print('validation results:+++++++++++++++++++++++++++++')\n",
    "            cvscores = []\n",
    "            cvscores_train = []  \n",
    "            for train_index, test_index in kfold.split(X_train_rotate[:1200], target_train[:1200]):\n",
    "                gmodel=Sequential()\n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                gmodel.add(Dropout(drop))\n",
    "    \n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                gmodel.add(Dropout(drop))\n",
    "    \n",
    "                gmodel.add(Flatten())\n",
    "    \n",
    "                gmodel.add(Dense(512))\n",
    "                gmodel.add(Activation('relu'))\n",
    "                gmodel.add(Dropout(0.2))\n",
    "    \n",
    "                gmodel.add(Dense(256))\n",
    "                gmodel.add(Activation('relu'))\n",
    "                gmodel.add(Dropout(0.2))\n",
    "    \n",
    "                gmodel.add(Dense(1))\n",
    "                gmodel.add(Activation('sigmoid'))\n",
    "\n",
    "                gmodel.compile(loss='binary_crossentropy', \n",
    "                               optimizer=Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                               metrics=['accuracy'])\n",
    "    \n",
    "                callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "                gmodel.fit(X_train_rotate[train_index], target_train[train_index], \n",
    "                           batch_size = batch_size,\n",
    "                           epochs = 100,\n",
    "                           verbose = 0,\n",
    "                           validation_data=(X_train_rotate[test_index], target_train[test_index]),\n",
    "                           callbacks=callbacks)\n",
    "                gmodel.load_weights(filepath=file_path)\n",
    "                scores = gmodel.evaluate(X_train_rotate[test_index], target_train[test_index], verbose=0)\n",
    "                scores_train = gmodel.evaluate(X_train_rotate[train_index], target_train[train_index], verbose=0)\n",
    "                print (\"lr=\",lr)\n",
    "                print (\"drop=\",drop)\n",
    "                print(\"batch_size=\",batch_size)\n",
    "                print('training:', gmodel.metrics_names[0], scores_train[0], gmodel.metrics_names[1], scores_train[1])\n",
    "                print('testing:', gmodel.metrics_names[0], scores[0], gmodel.metrics_names[1], scores[1])\n",
    "                cvscores.append(scores[0])\n",
    "                cvscores_train.append(scores_train[0])\n",
    "            print(\"lr=\",lr,\"drop=\",drop,\"batch_size=\",batch_size,\"loss==============\",\"%.2f (+/- %.2f)\" % (np.mean(cvscores), np.std(cvscores))) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train_rotate, target_train, random_state=1, train_size=0.9)\n",
    "#Without denoising, core features.\n",
    "\n",
    "import os\n",
    "gmodel=getModel()\n",
    "history = gmodel.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=24,\n",
    "          epochs=20,\n",
    "          verbose=0,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gmodel.load_weights(filepath=filepath)\n",
    "score = gmodel.evaluate(X_train_rotate, target_train, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import testing data\n",
    "X_test = np.load(\"/home/ubuntu/X_test.npy\")\n",
    "test_id = np.load('/home/ubuntu/test_id.npy')\n",
    "predicted_test=gmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_id\n",
    "submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is built with these previous contributors:\n",
    "\n",
    "https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d Keras Model for Beginners (0.210 on LB)+EDA+R&D by DeveshMaheshwari\n",
    "https://www.kaggle.com/toregil/welcome-to-deep-learning-cnn-99 Welcome to deep learning (CNN 99%) by Peter Grenholm\n",
    "https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/ Display Deep Learning Model Training History in Keras by Jason Brownlee\n",
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/ Evaluate the Performance Of Deep Learning Models in Keras by Jason Brownlee\n",
    "https://machinelearningmastery.com/check-point-deep-learning-models-keras/ Add callbacks for best evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
