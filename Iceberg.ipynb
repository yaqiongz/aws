{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "from scipy.ndimage.filters import median_filter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rotate = np.load(\"/home/ubuntu/X_train_rotate.npy\")\n",
    "target_train = np.load(\"/home/ubuntu/target_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(filepath, patience=5):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "file_path = \"model_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.229035266808 acc 0.902052481164\n",
      "testing: loss 0.289307525192 acc 0.883696780893\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.227391718452 acc 0.900753442453\n",
      "testing: loss 0.397319652767 acc 0.855659397715\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.210154418497 acc 0.910626136659\n",
      "testing: loss 0.281237755936 acc 0.863966770509\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.236198890124 acc 0.90779220773\n",
      "testing: loss 0.292560074997 acc 0.873180873181\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 7.56725966888 acc 0.53051155544\n",
      "testing: loss 7.56426754266 acc 0.530697190427\n",
      "lr= 0.001 drop= 0.2 batch_size= 72 loss============== 1.76 (+/- 2.90)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.18744264066 acc 0.919199792154\n",
      "testing: loss 0.287227175691 acc 0.877466251298\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.276017751921 acc 0.877890361148\n",
      "testing: loss 0.308651560672 acc 0.874350986501\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 1.4872403107 acc 0.560924915562\n",
      "testing: loss 1.44830073966 acc 0.565939771547\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 7.56503859296 acc 0.530649350649\n",
      "testing: loss 7.57315921606 acc 0.530145530146\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 64\n",
      "training: loss 0.224763707476 acc 0.903142041106\n",
      "testing: loss 0.339861833422 acc 0.832466181061\n",
      "lr= 0.001 drop= 0.2 batch_size= 64 loss============== 1.99 (+/- 2.83)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 7.56700406869 acc 0.530527409717\n",
      "testing: loss 7.56529504445 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.211029783638 acc 0.908287866994\n",
      "testing: loss 0.323201302992 acc 0.874350986501\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.270612203059 acc 0.881267861813\n",
      "testing: loss 0.329243976472 acc 0.862928348972\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.250834463997 acc 0.890909090847\n",
      "testing: loss 0.313935631265 acc 0.862785862786\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 48\n",
      "training: loss 0.392290694842 acc 0.819787068263\n",
      "testing: loss 0.472611538453 acc 0.79604578564\n",
      "lr= 0.001 drop= 0.2 batch_size= 48 loss============== 1.80 (+/- 2.88)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.193475317308 acc 0.919459599896\n",
      "testing: loss 0.253576080995 acc 0.897196261744\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.210963598452 acc 0.906988828267\n",
      "testing: loss 0.336852506035 acc 0.863966770571\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.200348911544 acc 0.92335671603\n",
      "testing: loss 0.282012678861 acc 0.896157840083\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.139944029582 acc 0.945714285652\n",
      "testing: loss 0.295085281443 acc 0.87525987526\n",
      "lr= 0.001\n",
      "drop= 0.2\n",
      "batch_size= 24\n",
      "training: loss 0.211894987416 acc 0.91716437289\n",
      "testing: loss 0.295807715106 acc 0.863683662851\n",
      "lr= 0.001 drop= 0.2 batch_size= 24 loss============== 0.29 (+/- 0.03)\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.195926445294 acc 0.914523252808\n",
      "testing: loss 0.352553728245 acc 0.855659397777\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 7.56700409412 acc 0.530527409717\n",
      "testing: loss 7.56529507614 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 7.56700408058 acc 0.530527409717\n",
      "testing: loss 7.56529511575 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.337227314742 acc 0.836623376561\n",
      "testing: loss 0.363178352184 acc 0.828482328482\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 72\n",
      "training: loss 0.308108243848 acc 0.867307192906\n",
      "testing: loss 0.371492541094 acc 0.852237252862\n",
      "lr= 0.001 drop= 0.4 batch_size= 72 loss============== 3.24 (+/- 3.53)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.280297509333 acc 0.877110937906\n",
      "testing: loss 0.343567148733 acc 0.840083073728\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 7.51427092383 acc 0.530527409717\n",
      "testing: loss 7.43849393288 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 7.49707605249 acc 0.530527409717\n",
      "testing: loss 7.45298364657 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 0.361976153169 acc 0.827012986951\n",
      "testing: loss 0.397335291033 acc 0.816008316008\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 64\n",
      "training: loss 7.56725963405 acc 0.53051155544\n",
      "testing: loss 7.56426752678 acc 0.530697190427\n",
      "lr= 0.001 drop= 0.4 batch_size= 64 loss============== 4.64 (+/- 3.49)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.246790366286 acc 0.889841517293\n",
      "testing: loss 0.344934659707 acc 0.858774662544\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.426079608645 acc 0.806703039797\n",
      "testing: loss 0.475699968334 acc 0.784008307435\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.186144830806 acc 0.919199792154\n",
      "testing: loss 0.292851375016 acc 0.863966770509\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.129047924311 acc 0.952467532406\n",
      "testing: loss 0.294749952349 acc 0.883575883576\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 48\n",
      "training: loss 0.325411176498 acc 0.841080238899\n",
      "testing: loss 0.378502046885 acc 0.825182101977\n",
      "lr= 0.001 drop= 0.4 batch_size= 48 loss============== 0.36 (+/- 0.07)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.335990921728 acc 0.842036892761\n",
      "testing: loss 0.334365606091 acc 0.841121495389\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.265346809114 acc 0.888022863097\n",
      "testing: loss 0.312886685675 acc 0.860851505711\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.186866814617 acc 0.926994024422\n",
      "testing: loss 0.247830441359 acc 0.897196261682\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.275965998312 acc 0.88\n",
      "testing: loss 0.315309042742 acc 0.864864864865\n",
      "lr= 0.001\n",
      "drop= 0.4\n",
      "batch_size= 24\n",
      "training: loss 0.292596879442 acc 0.876395741366\n",
      "testing: loss 0.315119807262 acc 0.864724245578\n",
      "lr= 0.001 drop= 0.4 batch_size= 24 loss============== 0.31 (+/- 0.03)\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.525411771093 acc 0.758118991946\n",
      "testing: loss 0.606478971221 acc 0.733125649013\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 7.16730548537 acc 0.530527409717\n",
      "testing: loss 7.19203775502 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 7.53723576295 acc 0.530527409717\n",
      "testing: loss 7.53761132292 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.269530716802 acc 0.881558441497\n",
      "testing: loss 0.269758394131 acc 0.881496881497\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 72\n",
      "training: loss 0.251536510527 acc 0.885484289857\n",
      "testing: loss 0.297772409765 acc 0.865764828304\n",
      "lr= 0.001 drop= 0.6 batch_size= 72 loss============== 3.18 (+/- 3.42)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.598035181132 acc 0.688750324822\n",
      "testing: loss 0.57433155634 acc 0.695742471443\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 5.15291599822 acc 0.530527409717\n",
      "testing: loss 5.36375283449 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.324969791582 acc 0.859444011478\n",
      "testing: loss 0.33127933524 acc 0.847352024922\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.359126193686 acc 0.832987012987\n",
      "testing: loss 0.366273896486 acc 0.836798336798\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 64\n",
      "training: loss 0.277521976888 acc 0.870682939481\n",
      "testing: loss 0.322287827529 acc 0.842872008325\n",
      "lr= 0.001 drop= 0.6 batch_size= 64 loss============== 1.39 (+/- 1.99)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 7.56700405414 acc 0.530527409717\n",
      "testing: loss 7.56529518705 acc 0.530633437175\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.291511841275 acc 0.875811899195\n",
      "testing: loss 0.342597844075 acc 0.828660436137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.28672034685 acc 0.879449207617\n",
      "testing: loss 0.298493447241 acc 0.882658359294\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 7.52065056354 acc 0.530649350649\n",
      "testing: loss 7.52237979014 acc 0.530145530146\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 48\n",
      "training: loss 0.23577391619 acc 0.892755128538\n",
      "testing: loss 0.280542304718 acc 0.87825182102\n",
      "lr= 0.001 drop= 0.6 batch_size= 48 loss============== 3.20 (+/- 3.55)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.285920444379 acc 0.871914783076\n",
      "testing: loss 0.306354173247 acc 0.857736240914\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.271834908154 acc 0.871135359834\n",
      "testing: loss 0.300363004888 acc 0.860851505711\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 8.45787255725 acc 0.469472590283\n",
      "testing: loss 8.45956298198 acc 0.469366562825\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.257968411686 acc 0.882077922016\n",
      "testing: loss 0.34751701702 acc 0.85343035343\n",
      "lr= 0.001\n",
      "drop= 0.6\n",
      "batch_size= 24\n",
      "training: loss 0.226829351624 acc 0.90132433142\n",
      "testing: loss 0.306638142576 acc 0.877211238293\n",
      "lr= 0.001 drop= 0.6 batch_size= 24 loss============== 1.94 (+/- 3.26)\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.380077707323 acc 0.821512081076\n",
      "testing: loss 0.39628190712 acc 0.833852544195\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.602333461891 acc 0.714471291291\n",
      "testing: loss 0.657789317247 acc 0.688473520249\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.309066834721 acc 0.86438035855\n",
      "testing: loss 0.31362791983 acc 0.860851505773\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.292025350719 acc 0.872467532437\n",
      "testing: loss 0.301114882972 acc 0.867983367983\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 72\n",
      "training: loss 0.320973621169 acc 0.849909114485\n",
      "testing: loss 0.358277665775 acc 0.826222684703\n",
      "lr= 0.001 drop= 0.8 batch_size= 72 loss============== 0.41 (+/- 0.13)\n",
      "batch_size= 64\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.431136083966 acc 0.809041309431\n",
      "testing: loss 0.471316226655 acc 0.784008307373\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.446520171016 acc 0.769290724926\n",
      "testing: loss 0.445618740866 acc 0.780893042637\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.600475031266 acc 0.696804364817\n",
      "testing: loss 0.594799236964 acc 0.703011422669\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.38907398597 acc 0.814285714317\n",
      "testing: loss 0.411602668565 acc 0.825363825364\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 64\n",
      "training: loss 0.353996411419 acc 0.829654635237\n",
      "testing: loss 0.383500297596 acc 0.836628511967\n",
      "lr= 0.001 drop= 0.8 batch_size= 64 loss============== 0.46 (+/- 0.07)\n",
      "batch_size= 48\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.372219085 acc 0.816056118472\n",
      "testing: loss 0.410470967724 acc 0.809968847414\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.388750986314 acc 0.79683034556\n",
      "testing: loss 0.394341711438 acc 0.797507788224\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.292567960239 acc 0.872174590803\n",
      "testing: loss 0.362315253228 acc 0.84942886812\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.344989472109 acc 0.838701298701\n",
      "testing: loss 0.345316387031 acc 0.83367983368\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 48\n",
      "training: loss 0.337204827833 acc 0.840041547619\n",
      "testing: loss 0.371831398663 acc 0.809573361082\n",
      "lr= 0.001 drop= 0.8 batch_size= 48 loss============== 0.38 (+/- 0.02)\n",
      "batch_size= 24\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.290844952752 acc 0.885424785674\n",
      "testing: loss 0.318635533253 acc 0.846313603323\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.360143135092 acc 0.821771888802\n",
      "testing: loss 0.404731627194 acc 0.823468328141\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.331655726352 acc 0.848012470787\n",
      "testing: loss 0.347671450422 acc 0.836967808961\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.349204508229 acc 0.827272727211\n",
      "testing: loss 0.381091275058 acc 0.829521829522\n",
      "lr= 0.001\n",
      "drop= 0.8\n",
      "batch_size= 24\n",
      "training: loss 0.302225693136 acc 0.853804206684\n",
      "testing: loss 0.328431937561 acc 0.840790842872\n",
      "lr= 0.001 drop= 0.8 batch_size= 24 loss============== 0.36 (+/- 0.03)\n",
      "ir= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "validation results:+++++++++++++++++++++++++++++\n",
      "lr= 0.0001\n",
      "drop= 0.2\n",
      "batch_size= 72\n",
      "training: loss 0.132989658402 acc 0.950636528969\n",
      "testing: loss 0.320640893926 acc 0.860851505773\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n\nCaused by op 'training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ce89fb0a78f4>\", line 54, in <module>\n    callbacks=callbacks)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 960, in fit\n    validation_steps=validation_steps)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 1634, in fit\n    self._make_train_function()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 555, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3083, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_169/MaxPool', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ce89fb0a78f4>\", line 21, in <module>\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3466, in pool2d\n    data_format=tf_data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1958, in max_pool\n    name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2806, in _max_pool\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce89fb0a78f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                            \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_rotate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                            callbacks=callbacks)\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mgmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_rotate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n\nCaused by op 'training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ce89fb0a78f4>\", line 54, in <module>\n    callbacks=callbacks)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 960, in fit\n    validation_steps=validation_steps)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 1634, in fit\n    self._make_train_function()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 555, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3083, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_169/MaxPool', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-ce89fb0a78f4>\", line 21, in <module>\n    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3466, in pool2d\n    data_format=tf_data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1958, in max_pool\n    name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2806, in _max_pool\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[72,64,71,71]\n\t [[Node: training_84/Adam/gradients/max_pooling2d_169/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_169/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_338/cond/Merge, max_pooling2d_169/MaxPool, training_84/Adam/gradients/AddN_15)]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "\n",
    "for lr in [1e-3,1e-4,1e-5]:\n",
    "    print (\"ir=\",lr)\n",
    "    for drop in [0.2, 0.4, 0.6, 0.8]:\n",
    "        print (\"drop=\",drop)\n",
    "        for batch_size in [72, 64, 48, 24]:\n",
    "            print(\"batch_size=\",batch_size)\n",
    "            print('validation results:+++++++++++++++++++++++++++++')\n",
    "            cvscores = []\n",
    "            cvscores_train = []\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "            for train_index, test_index in kfold.split(X_train_rotate, target_train):\n",
    "                gmodel=Sequential()\n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                gmodel.add(Dropout(drop))\n",
    "    \n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "                gmodel.add(BatchNormalization())\n",
    "                gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                gmodel.add(Dropout(drop))\n",
    "    \n",
    "                gmodel.add(Flatten())\n",
    "    \n",
    "                gmodel.add(Dense(512))\n",
    "                gmodel.add(Activation('relu'))\n",
    "                gmodel.add(Dropout(0.2))\n",
    "    \n",
    "                gmodel.add(Dense(256))\n",
    "                gmodel.add(Activation('relu'))\n",
    "                gmodel.add(Dropout(0.2))\n",
    "    \n",
    "                gmodel.add(Dense(1))\n",
    "                gmodel.add(Activation('sigmoid'))\n",
    "\n",
    "                gmodel.compile(loss='binary_crossentropy', \n",
    "                               optimizer=Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                               metrics=['accuracy'])\n",
    "    \n",
    "                callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "                gmodel.fit(X_train_rotate[train_index], target_train[train_index], \n",
    "                           batch_size = batch_size,\n",
    "                           epochs = 10,\n",
    "                           verbose = 0,\n",
    "                           validation_data=(X_train_rotate[test_index], target_train[test_index]),\n",
    "                           callbacks=callbacks)\n",
    "                gmodel.load_weights(filepath=file_path)\n",
    "                scores = gmodel.evaluate(X_train_rotate[test_index], target_train[test_index], verbose=0)\n",
    "                scores_train = gmodel.evaluate(X_train_rotate[train_index], target_train[train_index], verbose=0)\n",
    "                print (\"lr=\",lr)\n",
    "                print (\"drop=\",drop)\n",
    "                print(\"batch_size=\",batch_size)\n",
    "                print('training:', gmodel.metrics_names[0], scores_train[0], gmodel.metrics_names[1], scores_train[1])\n",
    "                print('testing:', gmodel.metrics_names[0], scores[0], gmodel.metrics_names[1], scores[1])\n",
    "                cvscores.append(scores[0])\n",
    "                cvscores_train.append(scores_train[0])\n",
    "            print(\"lr=\",lr,\"drop=\",drop,\"batch_size=\",batch_size,\"loss==============\",\"%.2f (+/- %.2f)\" % (np.mean(cvscores), np.std(cvscores))) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train_rotate, target_train, random_state=1, train_size=0.9)\n",
    "#Without denoising, core features.\n",
    "\n",
    "import os\n",
    "gmodel=getModel()\n",
    "history = gmodel.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=24,\n",
    "          epochs=20,\n",
    "          verbose=0,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gmodel.load_weights(filepath=filepath)\n",
    "score = gmodel.evaluate(X_train_rotate, target_train, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import testing data\n",
    "X_test = np.load(\"/home/ubuntu/X_test.npy\")\n",
    "test_id = np.load('/home/ubuntu/test_id.npy')\n",
    "predicted_test=gmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_id\n",
    "submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is built with these previous contributors:\n",
    "\n",
    "https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d Keras Model for Beginners (0.210 on LB)+EDA+R&D by DeveshMaheshwari\n",
    "https://www.kaggle.com/toregil/welcome-to-deep-learning-cnn-99 Welcome to deep learning (CNN 99%) by Peter Grenholm\n",
    "https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/ Display Deep Learning Model Training History in Keras by Jason Brownlee\n",
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/ Evaluate the Performance Of Deep Learning Models in Keras by Jason Brownlee\n",
    "https://machinelearningmastery.com/check-point-deep-learning-models-keras/ Add callbacks for best evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
